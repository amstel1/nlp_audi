{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лемма:  москва\n",
      "Лемма:  май\n",
      "Лемма:  метрополитен\n",
      "Лемма:  работать\n",
      "Лемма:  круглосуточно\n",
      "Лемма:  текст\n",
      "Лемма:  нужно\n",
      "Лемма:  передавать\n",
      "Лемма:  функция\n",
      "Лемма:  вид\n",
      "Лемма:  строка\n",
      "['текст_NOUN', 'нужно_ADV', 'передавать_VERB', 'функция_NOUN', 'вид_NOUN', 'строка_NOUN']\n"
     ]
    }
   ],
   "source": [
    "from PyEMD import EMD\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "from gensim.similarities import WmdSimilarity\n",
    "#from gensim.models.keyedvectors import load_word2vec_format\n",
    "from deeppavlov import build_model, configs\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "mapping = {\n",
    "'A':       'ADJ',                                                                                                                                                                                                                                                                    \n",
    "'ADV':     'ADV',                                                                                                                                                                                                                                                                    \n",
    "'ADVPRO':  'ADV',                                                                                                                                                                                                                                                                    \n",
    "'ANUM':    'ADJ',                                                                                                                                                                                                                                                                    \n",
    "'APRO':    'DET',                                                                                                                                                                                                                                                                    \n",
    "'COM':     'ADJ',                                                                                                                                                                                                                                                                    \n",
    "'CONJ':    'SCONJ',                                                                                                                                                                                                                                                                  \n",
    "'INTJ':    'INTJ',                                                                                                                                                                                                                                                                   \n",
    "'NONLEX':  'X' ,                                                                                                                                                                                                                                                                     \n",
    "'NUM':     'NUM' ,                                                                                                                                                                                                                                                                   \n",
    "'PART':    'PART',                                                                                                                                                                                                                                                                   \n",
    "'PR':      'ADP' ,                                                                                                                                                                                                                                                                   \n",
    "'S':       'NOUN' ,                                                                                                                                                                                                                                                                  \n",
    "'SPRO':    'PRON' ,                                                                                                                                                                                                                                                                  \n",
    "'UNKN':    'X'   ,                                                                                                                                                                                                                                                                   \n",
    "'V':       'VERB'\n",
    "}\n",
    "\n",
    "def tag_mystem(text='Текст нужно передать функции в виде строки!'):  \n",
    "    m = Mystem()\n",
    "    processed = m.analyze(text)\n",
    "    tagged = []\n",
    "    for w in processed:\n",
    "        \n",
    "        try:\n",
    "            lemma = w[\"analysis\"][0][\"lex\"].lower().strip()\n",
    "            if lemma not in russian_stopwords:\n",
    "                print('Лемма: ' , lemma)\n",
    "                pos = w[\"analysis\"][0][\"gr\"].split(',')[0]\n",
    "                pos = pos.split('=')[0].strip()\n",
    "                if pos in mapping:\n",
    "                    tagged.append(lemma + '_' + mapping[pos]) # здесь мы конвертируем тэги\n",
    "                else:\n",
    "                    tagged.append(lemma + '_X') # на случай, если попадется тэг, которого нет в маппинге\n",
    "        except KeyError:\n",
    "            continue # я здесь пропускаю знаки препинания, но вы можете поступить по-другому\n",
    "    return tagged\n",
    "\n",
    "#wasserstein_distance()\n",
    "\n",
    "\n",
    "\n",
    "#https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html\n",
    "\n",
    "#Вариант 1\n",
    "\n",
    "model  = KeyedVectors.load_word2vec_format('./models/180/model.bin', binary = True)\n",
    "model.init_sims(replace=True)  # Normalizes the vectors in the word2vec class.\n",
    "sentence_1='в Москве 9 мая метрополитен будет работать круглосуточно'\n",
    "#vec1 = model.tag_mystem(sentence_1)\n",
    "sentence_2 = \"метро в ночь на 10 мая будет работать круглосуточно\"\n",
    "#distance = model.similarity(preprocess_text(sentence_1), preprocess_text(sentence_2))  # Compute WMD as normal.\n",
    "#distance = model.wmdistance(tag_mystem(sentence_1), tag_mystem(sentence_2))\n",
    "\n",
    "tag_mystem('в Москве 9 мая метрополитен будет работать круглосуточно')\n",
    "\n",
    "#from pyemd import emd\n",
    "from gensim.similarities import WmdSimilarity\n",
    "\n",
    "'''\n",
    "text = \"Красивая мама красиво мыла раму\"\n",
    "m = Mystem()\n",
    "lemmas = m.lemmatize(text)\n",
    "print(lemmas)\n",
    "'''\n",
    "\n",
    "processed_mystem = tag_mystem()\n",
    "print(processed_mystem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'асфальт мимо цемент избегать зевака аплодисменты обитатель спальный аррондисман'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this kernel I'll show you how easy it is to preprocess the text in Russian.\n",
    "\n",
    "# You need to install two libraries:\n",
    "# * nltk - to get russian stopwords\n",
    "# * pymystem3 - for lemmatization\n",
    "\n",
    "# download stopwords corpus, you need to run it once\n",
    "import nltk\n",
    "#nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "#--------#\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "\n",
    "#Create lemmatizer and stopwords list\n",
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "#Preprocess function\n",
    "def preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "    \n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    return text\n",
    "\n",
    "#Examples    \n",
    "#preprocess_text(\"Ну что сказать, я вижу кто-то наступил на грабли, Ты разочаровал меня, ты был натравлен.\")\n",
    "#> 'сказать видеть кто-то наступать грабли разочаровывать натравлять'\n",
    "preprocess_text(\"По асфальту мимо цемента, Избегая зевак под аплодисменты. Обитатели спальных аррондисманов\")\n",
    "#> 'асфальт мимо цемент избегать зевака аплодисменты обитатель спальный аррондисман'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'сказать видеть кто-то наступать грабли разочаровывать натравлять'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(\"Ну что сказать, я вижу кто-то наступил на грабли, Ты разочаровал меня, ты был натравлен.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "russian_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Кинуть модель в нужную папку\n",
    "#разобраться с загрузкой модуля fastText\n",
    "#build_model(configs.ranking.paraphrase_ident_tune_interact, load_trained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'москва 9 май метрополитен работать круглосуточно'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(sentence_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6275603"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('метро_NOUN', 'метрополитен_NOUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './models/182/model.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-c1a64e8f79b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./models/182/model.bin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Normalizes the vectors in the word2vec class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deeppavlov_tf1_14\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1500\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1501\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1502\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1504\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deeppavlov_tf1_14\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m         \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deeppavlov_tf1_14\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m         \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m     )\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deeppavlov_tf1_14\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, ignore_ext, buffering, encoding, errors)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './models/182/model.bin'"
     ]
    }
   ],
   "source": [
    "model  = KeyedVectors.load_word2vec_format('./models/180/model.bin', binary = True)\n",
    "model.init_sims(replace=True)  # Normalizes the vectors in the word2vec class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лемма:  мама\n",
      "Лемма:  мыть\n",
      "Лемма:  рама\n",
      "Лемма:  папа\n",
      "Лемма:  отмывать\n",
      "Лемма:  рама\n",
      "0.5423087430818069\n"
     ]
    }
   ],
   "source": [
    "#https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html\n",
    "\n",
    "#Вариант 1\n",
    "\n",
    "\n",
    "\n",
    "sentence_1='мама мыла раму'\n",
    "sentence_2 = \"папа отмывал раму\"\n",
    "'''\n",
    "try:\n",
    "    distance = model.wmdistance(sentence_1, sentence_2)  # Compute WMD as normal.\n",
    "    print(1)\n",
    "except:\n",
    "    distance = model.wmdistance(preprocess_text(sentence_1), preprocess_text(sentence_2))\n",
    "    print(2)\n",
    "finally:\n",
    "    distance = model.wmdistance(tag_mystem(sentence_1), tag_mystem(sentence_2))\n",
    "    print(3)\n",
    "'''\n",
    "distance = model.wmdistance(tag_mystem(sentence_1), tag_mystem(sentence_2))\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_word2vec_format' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-7c4b77140abd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Вариант 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./models/180/model.bin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mnum_best\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcommon_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWmdSimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommon_texts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_best\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_word2vec_format' is not defined"
     ]
    }
   ],
   "source": [
    "#Вариант 2\n",
    "model  = load_word2vec_format('./models/180/model.bin', binary = True)\n",
    "num_best = 10\n",
    "common_texts = ['']\n",
    "index = WmdSimilarity(common_texts, model, num_best)\n",
    "query = ['дерево']\n",
    "#query = ['дерево_NOUN']\n",
    "sims = index[query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
